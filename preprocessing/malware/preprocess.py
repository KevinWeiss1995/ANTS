import pandas as pd
import numpy as np
import os
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split


"""
This script preprocesses malware traffic data to prepare it for machine learning tasks, 
including classification of attack and normal traffic. It performs the following steps:

1. **Identify Base Directory**:
   - Dynamically determines the root of the Git repository to establish a consistent 
     base directory for file paths.

2. **Load and Label Data**:
   - Loads attack and normal traffic data from CSV files.
   - Labels attack traffic with `1` and normal traffic with `0`.

3. **Handle Missing Values**:
   - Fills missing values in the combined dataset using the mean of each feature.

4. **Normalize Features**:
   - Applies Min-Max scaling to normalize the feature values to a range of [0, 1].

5. **Feature Selection**:
   - Trains a `RandomForestClassifier` on the dataset to calculate feature importances.
   - Selects features with importance above a specified threshold (e.g., 0.01).

6. **Save Processed Data**:
   - Saves the selected important features to a CSV file (`important_features.csv`).
   - Saves the normalized and processed dataset to another CSV file (`processed_data.csv`).

**Key Functions**:
- `get_base_directory()`: Identifies the root directory of the Git repository to 
  ensure consistent file path handling.
- `preprocess_data()`: Coordinates the entire preprocessing workflow, including 
  data loading, normalization, feature selection, and saving processed outputs.

**Input Files**:
- `data/malware/CTU13_Attack_Traffic.csv`: Contains attack traffic data.
- `data/malware/CTU13_Normal_Traffic.csv`: Contains normal traffic data.

**Output Files**:
- `data/malware/important_features.csv`: CSV file containing selected important features.
- `data/malware/processed_data.csv`: CSV file containing the normalized and processed dataset.

**Usage**:
Run this script directly to preprocess the malware traffic data. Ensure the required 
input files are present in the `data/malware/` directory within the Git repository.

**Dependencies**:
- `pandas`: For data manipulation.
- `numpy`: For numerical operations.
- `os`: For file and path handling.
- `sklearn.ensemble.RandomForestClassifier`: For feature importance calculation.
- `sklearn.model_selection.train_test_split`: For splitting the dataset during feature selection.
"""


def get_base_directory():
    """Dynamically determine the base directory of the git repository."""
    # Get the current working directory
    current_dir = os.getcwd()
    
    # Traverse up the directory tree to find the .git directory
    while current_dir != os.path.dirname(current_dir):
        if os.path.exists(os.path.join(current_dir, '.git')):
            return current_dir
        current_dir = os.path.dirname(current_dir)
    
    raise FileNotFoundError("No git repository found.")

def preprocess_data():
    # Determine the base directory of the git repository
    base_dir = get_base_directory()
    
    # Define file paths
    attack_file_path = os.path.join(base_dir, 'data', 'malware', 'CTU13_Attack_Traffic.csv')
    normal_file_path = os.path.join(base_dir, 'data', 'malware', 'CTU13_Normal_Traffic.csv')

    # Check if files exist
    if not os.path.exists(attack_file_path):
        raise FileNotFoundError(f"Attack data file not found: {attack_file_path}")
    if not os.path.exists(normal_file_path):
        raise FileNotFoundError(f"Normal data file not found: {normal_file_path}")

    # Load the CSV files
    attack_data = pd.read_csv(attack_file_path)
    normal_data = pd.read_csv(normal_file_path)

    # Combine the datasets
    attack_data['Label'] = 1  # Label attack data
    normal_data['Label'] = 0  # Label normal data
    combined_data = pd.concat([attack_data, normal_data], ignore_index=True)

    # Handle missing values (example: fill with mean)
    combined_data.fillna(combined_data.mean(), inplace=True)

    # Normalize features (example: Min-Max scaling)
    normalized_data = (combined_data - combined_data.min()) / (combined_data.max() - combined_data.min())

    # Feature selection using Random Forest
    X = normalized_data.drop('Label', axis=1)
    y = normalized_data['Label']
    
    # Split the data for feature importance calculation
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Fit Random Forest model
    model = RandomForestClassifier()
    model.fit(X_train, y_train)

    # Get feature importances
    feature_importances = model.feature_importances_
    feature_names = X.columns
    important_features = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})
    
    # Select features with importance above a certain threshold (e.g., 0.01)
    important_features = important_features[important_features['Importance'] > 0.01]
    
    # Save important features to a CSV file
    important_features.to_csv(os.path.join(base_dir, 'data', 'malware', 'important_features.csv'), index=False)

    # Save the processed data
    normalized_data.to_csv(os.path.join(base_dir, 'data', 'malware', 'processed_data.csv'), index=False)

# Call the preprocess function
if __name__ == "__main__":
    preprocess_data()
